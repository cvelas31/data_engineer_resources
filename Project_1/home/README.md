# Data Modeling with Postgres

Sparkify is a start up that wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

In this project we build an ETL pipeline usign Python and Postgres that transfers the information of two files into PostgreSQL. the two tables are the following:

**Song dataset:** 
Which is a is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset. 
```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```
And the content of a json is the following structure:
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```
**Log Dataset**
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

The log files in the dataset you'll be working with are partitioned by year and month. For example, here are filepaths to two files in this dataset.
````
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
````

Each json has a table with the following columns: 
``` 'artist', 'auth', 'firstName', 'gender', 'itemInSession', 'lastName', 'length', 'level', 'location', 'method', 'page', 'registration', 'sessionId', 'song', 'status', 'ts', 'userAgent', userId' ```

# Schema
Using the previous datasets, we are going to create 5 tables (1 Fact Table and 4 Dimension Tables)

## Fact Table
- songplays - records in log data associated with song plays i.e. records with page NextSong 
    - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
## Dimension Tables
- users - users in the app
    - user_id, first_name, last_name, gender, level
- songs - songs in music database
    - song_id, title, artist_id, year, duration
- artists - artists in music database
    - artist_id, name, location, latitude, longitude
- time - timestamps of records in songplays broken down into specific units
    - start_time, hour, day, week, month, year, weekday
    
# Discussion
    
With this structure the data is organized and can be used for analytics easily. It has a fact with the main information and in the dimension table we have some additional information that may be useful for deeper analysis.

With this structure its easy to do some analytics like counting the number of clients, looking some analytics of which type of customer are the ones that uses more the streaming service, look at which times of the day or the year are the more demanding ones opf services, look at the location where their services are used the most.

With this star schema the data is normalized in such a way that there is not so  much duplicated information. We have some additional resources about specific attributes. For ex: The artist name, location latitude and longitude are not so relevant. So having them in the fact or main tabkle is not accurate as it will make things more uncomfortable and not easily to visualize.

The Fact table has the main information that concerns the business as it has some identifiers of the songs played. If we want to look further infromationwe could use some simple joins for example to obtain specific day of the week or similar things. Also we can group by customer and have some analytics as mean time used. Most common day used, number of songs listened, etc.

# Execution of pipeline
To execute all the ETL pipeline you have to execute the following steps in the terminal:
```
python create_tables.py
```

With this step you have created the corresponding tables
Then you run:
```
python etl.py
```
With this step you will have the database created with the corresponding filled.

*Note: There is only one register in the songsplayed with song:_id and artist_id as it is a subset of the complete dataset*